{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbaweja/dbaweja.github.io/blob/main/TensorFlow_Advacned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj7ywgI79Dkd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY2K1HrG9Dm6"
      },
      "source": [
        "### 5. Reshaping and Broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8DqitCM9Dm8"
      },
      "source": [
        "#### 5.1. Reshaping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4w_-62t9Dm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00664f7a-95ee-4c63-cf29-34a7bea93448"
      },
      "source": [
        "# Reshaping changes the dimensions or axes and assigns the cell values in proper locations in the new tensor\n",
        "x = tf.Variable([[-3, 2, 4], [2, 0, -2], [5, 2, 3]], dtype=tf.float32)\n",
        "print(\"Initial tensor:\", x)\n",
        "x_reshaped = tf.reshape(x, shape=(9, 1)) # a (3, 3) tensor can be reshaped to (9, 1) amd (1, 9) tensors only\n",
        "print(\"Reshaped tensor:\", x_reshaped)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial tensor: <tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
            "array([[-3.,  2.,  4.],\n",
            "       [ 2.,  0., -2.],\n",
            "       [ 5.,  2.,  3.]], dtype=float32)>\n",
            "Reshaped tensor: tf.Tensor(\n",
            "[[-3.]\n",
            " [ 2.]\n",
            " [ 4.]\n",
            " [ 2.]\n",
            " [ 0.]\n",
            " [-2.]\n",
            " [ 5.]\n",
            " [ 2.]\n",
            " [ 3.]], shape=(9, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmNExRTM9DnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a33e60b-5b98-4b43-a3f7-19621279a05a"
      },
      "source": [
        "# tf.reshape raises exception if the new shape is incompatible\n",
        "try:\n",
        "    x_reshape = tf.reshape(x, shape=(3, 4))\n",
        "except Exception as e:\n",
        "    print(\"Exception raised while reshaping:\\n\", str(e))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception raised while reshaping:\n",
            " Input to reshape is a tensor with 9 values, but the requested shape has 12 [Op:Reshape]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxFN2uRe9DnQ"
      },
      "source": [
        "#### 5.2. Broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an6USxre9DnS"
      },
      "source": [
        "##### Mathematical operations on tensors requires shape compatibility otherwise exception is raised. However, broadcasting can resolve shape incompatibilities in few situations. Let us see an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TmTYx0U9DnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d4c514-a7a5-4ec5-9751-74d4c22454a4"
      },
      "source": [
        "x = tf.Variable([[3, 2, 4], [2, 0, 2], [4, 2, 3]], dtype=tf.float32)\n",
        "k = tf.Variable([2.1])\n",
        "\n",
        "print(\"Shape of x:\", x.shape)\n",
        "print(\"Shape of k:\", k.shape)\n",
        "\n",
        "# element-wise multiplication by broadcasting which repeats the only element in k along both the axes\n",
        "# to resolve the shape imcompatibility\n",
        "y = x * k\n",
        "print(\"Shape of y:\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x: (3, 3)\n",
            "Shape of k: (1,)\n",
            "Shape of y: tf.Tensor(\n",
            "[[6.2999997 4.2       8.4      ]\n",
            " [4.2       0.        4.2      ]\n",
            " [8.4       4.2       6.2999997]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqGTr6_Z9Dnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c678b274-e055-4628-9415-782a0de07afc"
      },
      "source": [
        "# Let us see another example\n",
        "k = tf.Variable([2.1, 3.0, -1.4])\n",
        "\n",
        "# in this case broadcasting repeats [2.1, 3.0, -1.4] three times along the row to make the shapes compatible\n",
        "y = x * k\n",
        "print(\"y:\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y: tf.Tensor(\n",
            "[[ 6.2999997  6.        -5.6      ]\n",
            " [ 4.2        0.        -2.8      ]\n",
            " [ 8.4        6.        -4.2      ]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo1EztuT9Dnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728877a5-bd52-4f7f-efe1-91abe2b30d5e"
      },
      "source": [
        "# Broadcasting of rank 2 tensor for multiplying with rank 3 tensor\n",
        "\n",
        "# Rank 2 tensor to be broadcasted\n",
        "k = tf.Variable([[2.1, 3.0, -1.4], [1.1, 3.2, -1.1], [0.67, 2.1, -0.03]])\n",
        "\n",
        "# Rank 3 tensor\n",
        "x = tf.Variable([[[3, 2, 4],\n",
        "                  [2, 0, 2],\n",
        "                  [4, 2, 3]],\n",
        "                 [[0, -2, 1],\n",
        "                  [4, 0, 2],\n",
        "                  [6, -2, 3]]\n",
        "                 ], dtype=tf.float32)\n",
        "\n",
        "# Here shape of x is (2, 3, 3) and that of k is (3, 3). Hence broadcasting will add a new axes and repeat k along it.\n",
        "y = x * k\n",
        "print(\"y:\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y: tf.Tensor(\n",
            "[[[ 6.2999997  6.        -5.6      ]\n",
            "  [ 2.2        0.        -2.2      ]\n",
            "  [ 2.68       4.2       -0.09     ]]\n",
            "\n",
            " [[ 0.        -6.        -1.4      ]\n",
            "  [ 4.4        0.        -2.2      ]\n",
            "  [ 4.02      -4.2       -0.09     ]]], shape=(2, 3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bn2apUB9Dnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713abb87-8f3a-4aff-be10-209f848df38f"
      },
      "source": [
        "# Alternatively, we can add the new dimension before multiplying\n",
        "print(\"Old shape:\", k.shape)\n",
        "k = tf.expand_dims(k, axis=0) # insert a new axis along the first dimension to create a rank 3 tensor\n",
        "print(\"New shape:\", k.shape)\n",
        "\n",
        "y = x * k\n",
        "print(\"y:\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old shape: (3, 3)\n",
            "New shape: (1, 3, 3)\n",
            "y: tf.Tensor(\n",
            "[[[ 6.2999997  6.        -5.6      ]\n",
            "  [ 2.2        0.        -2.2      ]\n",
            "  [ 2.68       4.2       -0.09     ]]\n",
            "\n",
            " [[ 0.        -6.        -1.4      ]\n",
            "  [ 4.4        0.        -2.2      ]\n",
            "  [ 4.02      -4.2       -0.09     ]]], shape=(2, 3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTS7x13G9Dn7"
      },
      "source": [
        "### 6. Automatic Differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy8bcZug9Dn9"
      },
      "source": [
        "#### Gradients or partial derivatives are useful for training linear models and neural networks using gradient-based optimization\n",
        "#### TensorFlow's tf.GradientTape records computations and computes gradients in reverse-mode differentiation.\n",
        "#### Let us see how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E00VcTQN9Dn-"
      },
      "source": [
        "####  \n",
        "#### 6.1 How to use tf.GradientTape?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8zWxrBO9DoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326b365f-f9c3-4cac-a487-cb459c979e0c"
      },
      "source": [
        "# declare tensor for with respect to which gradient is required\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "# using tf.GradientTape context to perform computations and record them in tape\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 3.0 * tf.math.log(x)\n",
        "\n",
        "# compute gradients\n",
        "dy_dx = tape.gradient(y, x) # for y = log(x), dy/dx = 1/x\n",
        "print(\"Gradient dy_dx:\", dy_dx) # alternately doing dy_dx.numpy() returns the result as a numpy array which in this case is a single value"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient dy_dx: tf.Tensor(1.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Elo0THg9DoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de64d00-fb59-4cd2-ecb9-bfb4c2d25af1"
      },
      "source": [
        "# for multi-variate functions, gradients are partial derivatives\n",
        "x = tf.Variable(3.0)\n",
        "y = tf.Variable(4.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    r = x**2 + y**2\n",
        "\n",
        "dr_dx, dr_dy = tape.gradient(r, [x, y]) # the first argument is the target function and the second one is a list of variables\n",
        "print(\"dr_dx:\", dr_dx.numpy()) # ∂r/∂x = 2x\n",
        "print(\"dr_dy:\", dr_dy.numpy()) # ∂r/∂y = 2y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dr_dx: 6.0\n",
            "dr_dy: 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQgEyIT79DoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68dbc09-13a6-44c4-a5c7-995684dba5ba"
      },
      "source": [
        "# tf.GradientTape supports the Chain Rule of Differentiation as well\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x ** 2 # y is a function of x\n",
        "    r = y ** 2 # r is a function of y\n",
        "\n",
        "dr_dx = tape.gradient(r, x) # Chain rule is applied to compute dr_dx as r is a function of x through y\n",
        "print(\"dr_dx:\", dr_dx.numpy()) # dr/dx = dr/dy × dy/dx = 2y × 2x = 2(x²) × 2x = 4x³"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dr_dx: 108.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7I5bvD69DoZ"
      },
      "source": [
        "#### Note that the intermediate variable 'y' is released outside the context of tf.GradientTape. To compute gradient dr/dy, tf.GradientTape\n",
        "#### needs to be persistent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1AKwufZ9Dod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf44bf4-14a7-433e-fceb-13620069a091"
      },
      "source": [
        "# Persistent tf.GradientTape\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    y = x ** 2\n",
        "    r = y ** 2\n",
        "\n",
        "dr_dx = tape.gradient(r, x)\n",
        "dr_dy = tape.gradient(r, y)\n",
        "\n",
        "print(\"dr_dx:\", dr_dx.numpy())\n",
        "print(\"dr_dy:\", dr_dy.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dr_dx: 108.0\n",
            "dr_dy: 18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn26EzYT9Dom"
      },
      "source": [
        "####  \n",
        "#### 6.2 How to use tf.GradientTape for model training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rTeP2dh9Dor"
      },
      "source": [
        "# Let us see this in action in the context of Logistic Regression\n",
        "\n",
        "# Points to note:\n",
        "# 1. For logistic regression, the probabilty P(y=1|X) = 1/[1+exp{-(WX+b)}]; where W & b are weights and bias of the model\n",
        "# 2. The loss function is L = -ylog[P(y=1|X)] - (1-y)log[1-P(y=1|X)] otherwise known as binary log loss.\n",
        "# 3. For training the model using Stochastic Gradient Descent, we need to compute ∂L/∂W and ∂L/∂b.\n",
        "\n",
        "# Let us see how we can compute the gradients using tf.GradientTape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_qnWPka9Doz"
      },
      "source": [
        "# declare weights and bias of the model\n",
        "num_features = 10 # the model has 10 features\n",
        "W = tf.Variable(tf.random.normal(shape=(10, ))) # wrapping the random numbers with tf.Variable to enable gradients on W\n",
        "b = tf.Variable(tf.random.normal(shape=(1, )))  # wrapping the random numbers with tf.Variable to enable gradients on b\n",
        "\n",
        "# let us initialize a training samples\n",
        "num_samples = 100\n",
        "\n",
        "# generate training samples randomly and wrap them up in tf.constant to disable gradient computation on samples\n",
        "X = tf.constant(tf.random.uniform(shape=(100, 10)))\n",
        "y = tf.constant(tf.cast(tf.greater(tf.random.uniform(shape=(100,)), 0.80), dtype=tf.float32)) # target must be 0 and 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCgO3mxn9Do8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d2df61-b224-4e61-ca20-73873f021516"
      },
      "source": [
        "# compute gradients\n",
        "with tf.GradientTape() as tape:\n",
        "    log_odds = tf.reduce_sum(W * X, axis=1) + b\n",
        "    # tf.reduce_sum(W * X, axis=1) results in a (1, ) vector to which b is added\n",
        "\n",
        "    probas = tf.sigmoid(log_odds) # using sigmoid function available in TensorFlow\n",
        "\n",
        "    loss = tf.reduce_mean(-y*tf.math.log(probas) -(1.-y)*tf.math.log(1.-probas))\n",
        "\n",
        "[dloss_dW, dloss_db]  = tape.gradient(loss, [W, b])\n",
        "\n",
        "print(\"dloss_dW:\", dloss_dW.numpy())\n",
        "print(\"dloss_db:\", dloss_db.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dloss_dW: [0.20875391 0.18559754 0.20375727 0.17207277 0.16018161 0.23883408\n",
            " 0.16166784 0.17670268 0.15549302 0.14451382]\n",
            "dloss_db: [0.3515248]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgAizW9v9DpE"
      },
      "source": [
        "####  \n",
        "#### 6.3 How to disable gradient computation for selected variables?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpaB-VGZ9DpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1717b8-7825-4ccf-f7bd-b66dfc53add5"
      },
      "source": [
        "# let us use an earlier example to show how to disable gradient computation\n",
        "# for multi-variate functions, gradients are partial derivatives\n",
        "x = tf.Variable(3.0, trainable=True)  # by default, trainable=True so we did npt mention it earlier\n",
        "y = tf.Variable(4.0, trainable=False) # trainable=False signals tf.GradientTape to stop tracking the variable\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    r = x**2 + y**2\n",
        "\n",
        "try:\n",
        "    dr_dx, dr_dy = tape.gradient(r, [x, y])\n",
        "    print(\"dr_dx:\", dr_dx.numpy())\n",
        "    print(\"dr_dy:\", dr_dy.numpy()) # dr_dy is None hence this line would raise an exception\n",
        "except Exception as e:\n",
        "    print(\"Error occurred during gradient computation:\\n\", str(e))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dr_dx: 6.0\n",
            "Error occurred during gradient computation:\n",
            " 'NoneType' object has no attribute 'numpy'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juWxatRjbbN3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}